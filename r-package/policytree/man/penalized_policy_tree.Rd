% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/penalized_policy_tree.R
\name{penalized_policy_tree}
\alias{penalized_policy_tree}
\title{Fit a penalized tree-based policy}
\usage{
penalized_policy_tree(
  X,
  Gamma1,
  Gamma2,
  depth = 2,
  split.step = 1,
  min.node.size = 1,
  penalty.type = c("ratio", "difference"),
  verbose = TRUE
)
}
\arguments{
\item{X}{The covariates used. Dimension \eqn{N*p} where \eqn{p} is the number of features.}

\item{Gamma1}{The rewards for each action. Dimension \eqn{N*d} where \eqn{d} is the number of actions.}

\item{Gamma2}{The corresponding penalities for each action. Dimension \eqn{N*d} where \eqn{d} is the number of actions.}

\item{depth}{The depth of the fitted tree. Default is 2.}

\item{split.step}{An optional approximation parameter, the number of possible splits
to consider when performing tree search. split.step = 1 (default) considers every possible split, split.step = 10
considers splitting at every 10'th sample and may yield a substantial speedup for dense features.
Manually rounding or re-encoding continuous covariates with very high cardinality in a
problem specific manner allows for finer-grained control of the accuracy/runtime tradeoff and may in some cases
be the preferred approach.}

\item{min.node.size}{An integer indicating the smallest terminal node size permitted. Default is 1.}

\item{penalty.type}{The type of penalty. Default is "ratio".}

\item{verbose}{Give verbose output. Default is TRUE.}
}
\value{
A policy_tree object.
}
\description{
Find a policy tree where the objective is penalized by some term \eqn{\Gamma_2}.
}
\details{
Let \eqn{\Gamma_{1,i}, \Gamma_{2,i} \in \mathbb R^d} and \eqn{\pi(X) \in \{0, 1\}^d}.
For penalty.type = "ratio", this function solves

\eqn{ \pi^* = argmax_{\pi \in \Pi}\left[ \frac{ \sum_{i=1}^{n} \pi(X_i) \cdot  \Gamma_{1,i} }{ max\left(1, \sqrt{\sum_{i=1}^{n} \pi(X_i) \cdot  \Gamma_{2,i}}\right) }\right]. }

For penalty.type = "difference", this function solves

\eqn{ \pi^* = argmax_{\pi \in \Pi}\left[ \sum_{i=1}^{n} \pi(X_i) \cdot  \Gamma_{1,i}  +\sqrt{\sum_{i=1}^{n} \pi(X_i) \cdot  \Gamma_{2,i}} \right]. }

When \eqn{\Gamma_2} is zero, this function's objective is identical to \code{policy_tree}.
}
\examples{
\donttest{
# Fit a penalized policy tree.
n <- 500
p <- 5
d <- 3
X <- round(matrix(rnorm(n * p), n, p), 2)
Y <- matrix(rnorm(n * d), n, d)
Y.ones <- matrix(1, n, d)

ppt <- penalized_policy_tree(X, Y, Y.ones, depth = 2)

# Predict treatment assignment.
pi <- predict(ppt, X)
}
}
