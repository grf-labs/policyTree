% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/penalized_policy_tree.R
\name{penalized_policy_tree}
\alias{penalized_policy_tree}
\title{Fit a penalized tree-based policy}
\usage{
penalized_policy_tree(
  X,
  Gamma1,
  Gamma2,
  penalty.type = c("sum", "ratio"),
  lambda = 1,
  depth = 2,
  split.step = 1,
  min.node.size = 1,
  verbose = TRUE
)
}
\arguments{
\item{X}{The covariates used. Dimension \eqn{N*p} where \eqn{p} is the number of features.}

\item{Gamma1}{The rewards for each action. Dimension \eqn{N*d} where \eqn{d} is the number of actions.}

\item{Gamma2}{The corresponding penalities for each action. Dimension \eqn{N*d} where \eqn{d} is the number of actions.}

\item{penalty.type}{The type of penalty. Default is "sum".}

\item{lambda}{An optional penalty parameter. Default is 1.}

\item{depth}{The depth of the fitted tree. Default is 2.}

\item{split.step}{An optional approximation parameter, the number of possible splits
to consider when performing tree search. split.step = 1 (default) considers every possible split, split.step = 10
considers splitting at every 10'th sample and may yield a substantial speedup for dense features.
Manually rounding or re-encoding continuous covariates with very high cardinality in a
problem specific manner allows for finer-grained control of the accuracy/runtime tradeoff and may in some cases
be the preferred approach.}

\item{min.node.size}{An integer indicating the smallest terminal node size permitted. Default is 1.}

\item{verbose}{Give verbose output. Default is TRUE.}
}
\value{
A policy_tree object.
}
\description{
Find a policy tree where the objective is penalized by some (positive) term \eqn{\Gamma_2}.
}
\details{
Let \eqn{\Gamma_{1,i}, \in \mathbb R^d, \Gamma_{2,i} \in \mathbb R^d_+} and \eqn{\pi(X) \in \{1, ..., d\}}.
For penalty.type = "sum", this function solves (Swaminathan and Joachims, 2015)

\eqn{ \pi^* = argmax_{\pi \in \Pi}\left[ \sum_{i=1}^{n} \Gamma_{1,i}(\pi(X_i)) - \lambda \sqrt{\sum_{i=1}^{n} \Gamma_{2,i}(\pi(X_i))} \right]. }

For penalty.type = "ratio", this function solves

\eqn{ \pi^* = argmax_{\pi \in \Pi}\left[ \frac{ |\sum_{i=1}^{n} \Gamma_{1,i}(\pi(X_i))|}{ max\left(\lambda, \sqrt{\sum_{i=1}^{n} \Gamma_{2,i}(\pi(X_i)}\right) }\right]. }

\eqn{ \lambda \in \mathbb R} is a penalty parameter. When this penalty is zero, the first objective is identical to \code{policy_tree}.
}
\examples{
\donttest{
# Fit a penalized policy tree.
n <- 500
p <- 5
d <- 3
X <- round(matrix(rnorm(n * p), n, p), 2)
Y <- matrix(rnorm(n * d), n, d)
Y.ones <- matrix(1, n, d)

ppt <- penalized_policy_tree(X, Y, Y.ones, depth = 2)

# Predict treatment assignment.
pi <- predict(ppt, X)
}
}
\references{
Swaminathan, Adith, and Thorsten Joachims.
"Batch learning from logged bandit feedback through counterfactual risk minimization."
The Journal of Machine Learning Research 16, no. 1 (2015): 1731-1755.
}
