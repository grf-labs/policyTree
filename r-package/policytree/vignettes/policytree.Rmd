---
title: "policytree introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{policytree introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document provides a short introduction to the `policytree` package, with examples from Zhou, Athey and Wager (2018), and Athey and Wager (2021). The last section addresses details, such as treatment estimates with one-vs-all grf, and the runtime of `policy_tree`.

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
set.seed(42)
```

```{r setup}
library(policytree)
library(grf)
```

# Ex. 1: Binary treatment effect estimation and policy learning
```{r}
n <- 10000
p <- 10

X <- matrix(rnorm(n * p), n, p)
ee <- 1 / (1 + exp(X[, 3]))
tt <- 1 / (1 + exp((X[, 1] + X[, 2]) / 2)) - 0.5
W <- rbinom(n, 1, ee)
Y <- X[, 3] + W * tt + rnorm(n)

cf <- causal_forest(X, Y, W)

plot(tt, predict(cf)$predictions)

dr <- double_robust_scores(cf)
tree <- policy_tree(X, dr, 2)
tree
pp <- predict(tree, X)
boxplot(tt ~ pp)
plot(tree)

plot(X[, 1], X[, 2], col = pp)
abline(0, -1, lwd = 4, col = 4)
```

# Ex. 2: Multi-action treatment effect estimation

The following example is from the 3-action DGP from section 6.4.1 in [Zhou, Athey and Wager (2018)](https://arxiv.org/abs/1810.04778)

```{r}
n <- 10000
p <- 10
data <- gen_data_mapl(n, p)
head(data.frame(data)[1:6])

X <- data$X
Y <- data$Y
W <- data$action

multi.forest <- multi_causal_forest(X, Y, W)

# tau.hats:
head(predict(multi.forest)$predictions)

# Each region with optimal action
region.pp <- data$region + 1
plot(X[, 5], X[, 7], col = region.pp)
leg <- sort(unique(region.pp))
legend("topleft", legend = leg - 1, col = leg, pch = 10)
```

### Policy learning

Cross-fitted Augmented Inverse Propensity Weighted Learning (CAIPWL) with the optimal depth 2 tree

```{r}
Gamma.matrix <- double_robust_scores(multi.forest)
head(Gamma.matrix)

train <- sample(1:n, 9000)
opt.tree <- policy_tree(X[train, ], Gamma.matrix[train, ], depth = 2)
opt.tree

plot(opt.tree)
```

Predict treatment on held out data

```{r}
X.test <- X[-train, ]
pp <- predict(opt.tree, X.test)
head(pp)

plot(X.test[, 5], X.test[, 7], col = pp)
leg <- sort(unique(pp))
legend("topleft", legend = leg - 1, col = leg, pch = 10)
```

# Ex. 3: Efficient Policy Learning - Binary Treatment and Instrumental Variables

The following example is from section 5.2 in [Wager and Athey (2021)](https://arxiv.org/abs/1702.02896).

```{r}
n <- 500
data <- gen_data_epl(n, type = "continuous")
head(data.frame(data))[1:6]

iv.forest <- instrumental_forest(X = data$X, Y = data$Y, W = data$W, Z = data$Z)

gamma <- double_robust_scores(iv.forest)
head(gamma)
```

Find the depth-2 tree which solves (2):

```{r}
train <- sample(1:400)
tree <- policy_tree(data$X[train, ], gamma[train, ])
tree
```

Evaluate the policy on held out data:

```{r}
piX <- predict(tree, data$X[-train, ]) - 1
head(piX)

reward.policy <- mean((2 * piX - 1) * data$tau[-train])
reward.policy
```

# Multi causal forest treatment estimates and baselines

This is a worked example of how `multi_causal_forest` treatment estimates can be decomposed (and interpreted) to target the parameter you are interested in. Consider the following DGP:


```{r}
n <- 10000
p <- 5
X <- matrix(rnorm(n * p), n, p)
W <- sample(c("A", "B", "C"), n, replace = TRUE)
tauB <- X[, 2]
tauC <- 2 * X[, 2]
Y <- X[, 1] + tauB * (W == "B") + tauC * (W == "C") + rnorm(n)
mcf <- multi_causal_forest(X, Y, W)
tau.mcf <- predict(mcf)$predictions
```

Recall that `multi_causal_forest` simply fits one `causal_forest` per treament, with the same nuisance component $Y.hat = m(x) = E[Y | X]$ for each forest. Each forest returns a treatment estimate  $\tau_k(x) = \frac{\mu_k(x) - m(x)}{1 - e_k(x)}$, where $\mu_k(x) = E[Y | X, W=W_k]$ is the conditional mean of arm $k$ and $e_k(x)$ is the assignment probability of arm $k$.

In the above DGP

$m(x) = X1 + 1/3 X2 + 1/3 \cdot 2X2 = X1 + X2$

$\mu_B = X1 + X2$

$\mu_C = X1 + 2X2$

$e_B(x) = e_C(x) = 1/3$

Which gives the multi causal forest predictions:

$\tau_B(X) = (X1 + X2 - (X1 + X2)) / (1 - 1/3) = 0$

$\tau_C(X) = (X1 + X2 - (X1 + 2X2)) / (1 - 1/3) = 3/2 X2$

I.e, the estimated $\tau_B(x)$ will be zero because of the centering step. The estimate for $\tau_C(X)$ should be scaled by $4/3$ in order to be compared with ground truth ($2 X2)$:

```{r}
plot(X[, 2], tauB, type = "l", lwd = 1)
points(X[, 2], tau.mcf[, "B"], col = 2, cex = 0.1)
lines(X[, 2], tauC, lwd = 3)
points(X[, 2], tau.mcf[, "C"], col = 3, cex = 0.1)
points(X[, 2], 4/3 * tau.mcf[, "C"], col = 4, cex = 0.1)
legend = c("treatment B", "tau.mcf[,'B']",
           "treatment C", "tau.mcf[,'C']", "4/3 tau.mcf[,'C'] ")
legend("topleft", legend = legend,
       col = c(1, 2, 1, 3, 4),
       pch = c(NA, 19, NA, 19, 19),
       bg = "transparent",
       bty = "n",
       lwd = c(1, NA, 3, NA, NA))
```

Naturally the following regression coefficient is close to $4/3$
```{r}
lm(tauC ~ tau.mcf[, "C"] - 1)
```

# Gauging the runtime of tree search

Exact tree search is intended as a way to find shallow (i.e. depth 2 or 3) globally optimal tree-based polices on datasets of "moderate" size. The amortized runtime of the exact tree search is $O(p^k n^k (log n + d) + pnlog n)$ where $p$ is the number of features, $n$ the number of observations, $d$ the number of treatments, and $k \geq 1$ the tree depth. Due to the exponents in this expression, exact tree search will not scale to datasets of arbitrary size.

As an example, the runtime of a depth two tree scales quadratically with the number of observations, implying that doubling the number of samples will quadruple the runtime. n refers to the number of distinct observations, substantial speedups can be grained when the features are discrete (with all binary features, the runtime will be ~ linear in n), and it is often beneficial to round down very dense data to a lower cardinality (the optional parameter split.step emulates this, though rounding/re-encoding allow for finer-grained control).

As a point of reference, the following table presents runtimes for some example problems.

| depth | n (continuous) | features | actions | split.step | time    |
|-------|----------------|----------|---------|------------|---------|
| 2     | 1000           | 30       | 20      | 1          | 1.5 min |
| 2     | 1000           | 30       | 20      | 10         | 7 sec   |
| 2     | 10 000         | 30       | 20      | 1          | 3 hrs   |
| 2     | 10 000         | 30       | 20      | 10         | 14 min  |
| 2     | 10 000         | 30       | 20      | 1, but `round(X, 2)`         | 8 min   |
| 2     | 100 000        | 30       | 20      | 10         | 50 hrs |
| 2     | 100 000        | 30       | 20      | 1, but `round(X, 2)`         | 6.3 hrs |
| 2     | 100 000        | 60       | 20      | 1, but `round(X, 2)`         | 25 hrs |
| 2     | 100 000        | 30       | 3       | 10         | 7.4 hrs |
